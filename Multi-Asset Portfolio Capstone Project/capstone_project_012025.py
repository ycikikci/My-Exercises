# -*- coding: utf-8 -*-
"""Yasemin_Cikikci_Capstone_Project_012025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kurbpKYyAe9bfpVl4mwbFfV1a-9AN-qW

# Import the data
"""

import yfinance as yf
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import linregress
from scipy.optimize import minimize

from datetime import datetime

import scipy.optimize as sco
from statsmodels.stats.outliers_influence import variance_inflation_factor
import pandas_datareader as web
import statsmodels.api as smf
import urllib.request
import zipfile

equity_etf= ["SPY", "EZU", "EWJ", "FXI", "QQQ","EEM","XLK", "XLY", "XLC", "XLI", "XLE", "XLB", "XLV", "XLRE", "XLU"]
bond_etf = ["SHY", "IEF","IGOV", "LQD", "HYG","TLT", "BND", "BIV", "VGLT", "TIP", "EMB", "VCIT", "IGLB", "SCHP", "VTEB"]
reit_etf = ["VNQ", "ICF", "SCHH", "RWR", "REM", "REET", "VNQI", "FREL", "ROOF", "IYR", "SRET", "NLY", "PSA"]
commodity_etf = ["BNO", "USO", "GLD", "SLV", "CANE","CORN", "WEAT","DBC", "GDX", "GDXJ", "VDE", "OIH", "DBA", "UCO", "SCCO"]

etf_tickers = [
   "SPY", "EZU", "EWJ", "FXI", "QQQ", "EEM", "XLK", "XLY", "XLC", "XLI", "XLE", "XLB", "XLV","XLRE", "XLU",  # Equity ETFs
   "SHY", "IEF", "IGOV", "LQD", "HYG", "TLT", "BND", "BIV", "VGLT", "TIP", "EMB","VCIT", "IGLB", "SCHP", "VTEB",  # Bond ETFs
   "VNQ", "ICF", "SCHH", "RWR", "REM", "REET", "VNQI", "FREL", "ROOF", "IYR", "SRET", "NLY", "PSA",  # REIT ETFs
   "BNO", "USO", "GLD", "SLV", "CANE","CORN", "WEAT", "DBC", "GDX", "GDXJ", "VDE", "XLE", "OIH", "DBA", "UCO", "SCCO" # Commodity ETFs
]

crypto_tickers = ["BTC-USD", "ETH-USD", "XRP-USD"]

RISKY_ASSETS = [
    "SPY", "EZU", "EWJ", "FXI", "QQQ", "EEM", "XLK", "XLY", "XLC", "XLI", "XLE", "XLB", "XLV", "XLRE", "XLU",
    "SHY", "IEF", "IGOV", "LQD", "HYG", "TLT", "BND", "BIV", "VGLT", "TIP", "EMB", "VCIT", "IGLB", "SCHP", "VTEB",
    "VNQ", "ICF", "SCHH", "RWR", "REM", "REET", "VNQI", "FREL", "ROOF", "IYR", "SRET", "NLY", "PSA",
    "BNO", "USO", "GLD", "SLV", "CANE", "CORN", "WEAT", "DBC", "GDX", "GDXJ", "VDE", "XLE", "OIH", "DBA", "UCO", "SCCO",
    "BTC-USD", "ETH-USD", "XRP-USD"
]

pricesData = yf.download(etf_tickers + crypto_tickers, start='2021-11-01', end='2024-11-01', interval="1wk")["Adj Close"]
pricesData_train = pricesData.loc['2021-11-01':'2024-02-26', :].copy()
pricesData_test = pricesData.loc['2024-02-26':].copy()

pricesData_train

pricesData_test

"""# Data - First Period"""

plt.figure(figsize=(35, 6))
plt.plot(pricesData_train[etf_tickers])
plt.title('Adjusted Close Prices of Selected Assets')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend(pricesData_train[etf_tickers].columns, loc='center left', bbox_to_anchor=(1.05, 0.5), ncol=1)  # Right side, vertically centered

plt.subplots_adjust(right=0.8)

plt.grid()
plt.show()
pricesData_train[etf_tickers].head(10)

plt.figure(figsize=(35, 6))
plt.plot(pricesData_train[crypto_tickers])
plt.title('Adjusted Close Prices of Selected Assets')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend(pricesData_train[crypto_tickers].columns, loc='center left', bbox_to_anchor=(1.05, 0.5), ncol=1)  # Right side, vertically centered

plt.subplots_adjust(right=0.8)


plt.grid()
plt.show()
pricesData_train[crypto_tickers].head(10)

pricesData_train.isna().sum()

returns_train = pricesData_train.pct_change().dropna()
returns_test = pricesData_test.pct_change().dropna()
returns_train.columns

"""#Variance Inflation Factor (VIF)"""

def calc_vif1(X):
   vif = pd.DataFrame()
   vif["Ticker"] = X.columns
   vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
   vif = vif.sort_values(by="VIF", ascending=True)
   return(vif)

pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)

control=calc_vif1(returns_train)
print(control)

def calc_vif2(X):
    vif = pd.DataFrame()
    vif["variables"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    columns_to_drop = vif[vif["VIF"] > 150]["variables"]
    X_dropped = X.drop(columns=columns_to_drop)
    return X_dropped

returns_train = calc_vif2(returns_train)
returns_train

control2=calc_vif1(returns_train)
print(control2)

def calc_vif3(X):
    vif = pd.DataFrame()
    vif["variables"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    columns_to_drop = vif[vif["VIF"] > 10]["variables"]
    X_dropped = X.drop(columns=columns_to_drop)
    return X_dropped

returns_train = calc_vif3(returns_train)
returns_train

control3=calc_vif1(returns_train)
print(control3)

"""#Asset Elimination"""

annualized_returns = returns_train.mean() * 52
returns_train_mean_sorted = annualized_returns.sort_values(ascending=False)
returns_train_mean_sorted

annualized_volatility = returns_train.std() * np.sqrt(52)
returns_train_vol_sorted = annualized_volatility.sort_values(ascending=False)
returns_train_vol_sorted

vif_df = pd.DataFrame(control3)
returns_df = pd.DataFrame(returns_train_mean_sorted)
vol_df=pd.DataFrame(returns_train_vol_sorted)
merged_df_vif_return = pd.merge(returns_df,vif_df, on='Ticker', how='left')
merged_df= pd.merge(merged_df_vif_return,vol_df, on='Ticker', how='left')
merged_df.rename(columns={'0_x': 'Returns', '0_y': 'Standard Deviations'}, inplace=True)
merged_df

"""### Depending to returns, vif values, standard deviations and market cap we choose assets for the portfolio construction. CANE has smallest market cap in the portfolio. Because of that it is eliminated. Also positive returns are our target. Negative ones are eliminated too. In addition that, multi-asset portfolio should cover geographical diversity. Japan, Emerging Market, Eurozone, China, United States create variety. EZU has a VIF value which is greater than 5. But for the geographical differentiation, a constraint that forces the model to keep the asset in the portfolio is added. XLK which symbolizes the technology is also added to the portfolio with that constraint to reach the desired diversity. ETH-USD has high return depending to other assets. Despite its high VIF value, it is also added in the constraint to keep in our portfolio. XRP-USD has highest standard deviation. It is eliminated for that high variance which could cause false predictions. Remaining tickers are 'BTC-USD', 'DBA', 'EMB', 'ETH-USD', 'EWJ', 'EZU', 'FXI', 'GLD', 'OIH', 'PSA', 'SCCO', 'XLC', 'XLK', 'XLU', 'XLV'."""

def calc_vif(X):
    vif = pd.DataFrame()
    vif["variables"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    vif.loc[vif["variables"].isin(["SHY","CORN","VTEB","XRP-USD","NLY","WEAT","VNQI","CANE","SLV"]), "VIF"] = 1000
    vif.loc[vif["variables"].isin(["EZU","ETH-USD","XLK"]), "VIF"] = 0
    columns_to_drop = vif[vif["VIF"] > 5]["variables"]
    X_dropped = X.drop(columns=columns_to_drop)
    return X_dropped

returns_train = calc_vif(returns_train)
returns_train

returns_train.columns

returns_test.columns

returns_test=returns_test.drop(columns=["BIV", "SLV","BND", "BNO", "CANE", "CORN", "DBC", "EEM", "FREL", "GDX", "GDXJ", "HYG", "ICF", "IEF", "IGLB", "IGOV", "IYR", "LQD", "NLY", "QQQ", "REET", "REM", "ROOF", "RWR", "SCHH", "SCHP", "SHY", "SLV", "SPY", "SRET", "TIP", "TLT", "UCO", "USO", "VCIT", "VDE", "VGLT", "VNQ", "VNQI", "VTEB", "WEAT", "XLB", "XLE", "XLI", "XLRE", "XLY", "XRP-USD"])
returns_test

remaining_columns = returns_train.columns
RISKY_ASSETS = [asset for asset in RISKY_ASSETS if asset in remaining_columns]

RISKY_ASSETS

remaining_columns = returns_train.columns
etf_tickers= [asset for asset in etf_tickers if asset in remaining_columns]

etf_tickers

remaining_columns = returns_train.columns
crypto_tickers= [asset for asset in crypto_tickers if asset in remaining_columns]

crypto_tickers

remaining_columns = returns_train.columns
equity_etf= [asset for asset in equity_etf if asset in remaining_columns]

equity_etf

remaining_columns = returns_train.columns
bond_etf= [asset for asset in bond_etf if asset in remaining_columns]

bond_etf

remaining_columns = returns_train.columns
reit_etf= [asset for asset in reit_etf if asset in remaining_columns]

reit_etf

remaining_columns = returns_train.columns
commodity_etf= [asset for asset in commodity_etf if asset in remaining_columns]

commodity_etf

n_assets =  len(returns_train.columns)
portfolio_weights = n_assets * [1 / n_assets]
portfolio_weights

portfolio_returns = pd.Series(np.dot(portfolio_weights, returns_train.T),index=returns_train.index)
portfolio_returns

N_PORTFOLIOS = 10 ** 5
N_WEEKS = 52
avg_returns = returns_train.mean() * N_WEEKS
cov_mat = returns_train.cov() * N_WEEKS

avg_returns

"""##Cumulative Returns"""

tickers=['BTC-USD', 'DBA', 'EMB', 'ETH-USD', 'EWJ', 'EZU', 'FXI', 'GLD', 'OIH', 'PSA', 'SCCO', 'XLC',
       'XLK', 'XLU', 'XLV']
cumulative_returns = (1 + returns_train).cumprod() - 1

def save_and_show_plot(data, title, ylabel, filename):
    plt.figure(figsize=(14, 7))
    for ticker in tickers:
        plt.plot(data[ticker])

    plt.title(title)
    plt.xlabel('Date')
    plt.ylabel(ylabel)
    plt.legend(tickers, loc='center left', bbox_to_anchor=(1.05, 0.5), ncol=1)

    plt.grid()

    plt.savefig(filename)
    plt.show()

save_and_show_plot(cumulative_returns, 'Cumulative Returns of Each Ticker', 'Cumulative Return', 'cumulative_returns.png')

"""##Annualized Returns vs Standard Deviations"""

annualized_returns = returns_train.mean() * 52

annualized_volatility = returns_train.std() * np.sqrt(52)

plt.figure(figsize=(12, 8))
plt.scatter(annualized_volatility, annualized_returns, color='blue', marker='o')

for i, ticker in enumerate(tickers):
    plt.text(annualized_volatility[i], annualized_returns[i], ticker, fontsize=9, ha='right', va='bottom')

plt.xlabel('Standard Deviation (Annualized Volatility)')
plt.ylabel('Average Annualized Returns')
plt.title('Scatter Plot of Average Annualized Returns vs Standard Deviation of Assets')

plt.grid(True)

plt.show()

"""##Correlation Matrix"""

plt.figure(figsize=(60,20))
sns.set(font_scale=1.4)
sns.heatmap(returns_train.corr(),cmap="coolwarm",annot=True)
plt.title("CorrelationMatrix",fontsize=24)
plt.show()

"""#Finding the Efficient Frontier with Simulation"""

from datetime import datetime
np.random.seed(int(np.round(datetime.now().timestamp(),0)))

weights_sim = np.random.lognormal(size=(N_PORTFOLIOS, n_assets))

weights_sim /= np.sum(weights_sim, axis=1)[:, np.newaxis]
weights_sim

portf_rtns_sim = np.dot(weights_sim, avg_returns)
portf_vol_sim = []
for i in range(0, len(weights_sim)):
    portf_vol_sim.append(np.sqrt(np.dot(weights_sim[i].T, np.dot(cov_mat, weights_sim[i]))))

portf_vol_sim = np.array(portf_vol_sim)
portf_sharpe_ratio_sim = portf_rtns_sim / portf_vol_sim

portf_results_sim_df = pd.DataFrame({'returns_sim': portf_rtns_sim,'volatility_sim': portf_vol_sim,'sharpe_ratio_sim':portf_sharpe_ratio_sim})
portf_results_sim_df

portf_results_sim_df.describe()

N_POINTS = 1000
portf_vol_sim_ef = []
indices_to_skip = []

portf_rtns_sim_ef = np.linspace(portf_results_sim_df.returns_sim.min(),portf_results_sim_df.returns_sim.max(),N_POINTS)

portf_rtns_sim_ef = np.round(portf_rtns_sim_ef, 2)
portf_rtns_sim = np.round(portf_rtns_sim, 2)

for point_index in range(N_POINTS):
    if portf_rtns_sim_ef[point_index] not in portf_rtns_sim:
        indices_to_skip.append(point_index)
        continue
    matched_ind = np.where(portf_rtns_sim ==portf_rtns_sim_ef[point_index])
    portf_vol_sim_ef.append(np.min(portf_vol_sim[matched_ind]))

portf_rtns_sim_ef = np.delete(portf_rtns_sim_ef, indices_to_skip)
portf_rtns_sim_ef

"""##Efficient Frontier"""

import matplotlib.pyplot as plt

MARKS = ['o', 'X', 'd', '*', 'v', 's', 'p','^','2', '3', '8', '_', '|', '4','>']

RISKY_ASSETS =['BTC-USD', 'DBA', 'EMB', 'ETH-USD', 'EWJ', 'EZU', 'FXI', 'GLD', 'OIH', 'PSA', 'SCCO', 'XLC',
       'XLK', 'XLU', 'XLV']
fig, ax = plt.subplots()
fig.set_figheight(8)
fig.set_figwidth(15)

portf_results_sim_df.plot(kind='scatter', x='volatility_sim', y='returns_sim', c='sharpe_ratio_sim', cmap='RdYlGn', edgecolors='black', ax=ax)
ax.set(xlabel='Volatility',ylabel='Expected Returns',title='Efficient Frontier')
ax.plot(portf_vol_sim_ef, portf_rtns_sim_ef, 'b--')

for asset_index in range(n_assets):
    ax.scatter(x=np.sqrt(cov_mat.iloc[asset_index, asset_index]),y=avg_returns[asset_index],
               marker=MARKS[asset_index],s=150,color='blue',
               label=RISKY_ASSETS[asset_index])

ax.legend(loc='upper left', bbox_to_anchor=(1.3, 1), prop={'size': 15})

# Adjust layout to ensure the legend doesn't overlap
plt.tight_layout()

# Show the plot
plt.show()

max_sharpe_ind = np.argmax(portf_results_sim_df.sharpe_ratio_sim)
max_sharpe_portf = portf_results_sim_df.loc[max_sharpe_ind]
min_vol_ind = np.argmin(portf_results_sim_df.volatility_sim)
min_vol_portf = portf_results_sim_df.loc[min_vol_ind]
min_vol_portf

print('Maximum Sharpe ratio portfolio ----')
print('Performance')

for index, value in max_sharpe_portf.items():
    print(f'{index}: {100 * value:.2f}% ', end="", flush=True)


print('\nWeights')

for x, y in zip(RISKY_ASSETS,weights_sim[np.argmax(portf_results_sim_df.sharpe_ratio_sim)]):
    print(f'{x}: {100*y:.2f}% ', end="", flush=True)

print('Minimum variance portfolio ----')
print('Performance')
for index, value in min_vol_portf.items():
    print(f'{index}: {100 * value:.2f}% ', end="", flush=True)


print('\nWeights')

for x, y in zip(RISKY_ASSETS,weights_sim[np.argmin(portf_results_sim_df.volatility_sim)]):
    print(f'{x}: {100*y:.2f}% ', end="", flush=True)

fig, ax = plt.subplots()
fig.set_figheight(8)
fig.set_figwidth(15)
portf_results_sim_df.plot(kind='scatter', x='volatility_sim',y='returns_sim', c='sharpe_ratio_sim',cmap='RdYlGn', edgecolors='black',ax=ax)
ax.scatter(x=max_sharpe_portf.volatility_sim,y=max_sharpe_portf.returns_sim,c='blue', marker='*',s=200, label='Max Sharpe Ratio')
ax.scatter(x=min_vol_portf.volatility_sim,y=min_vol_portf.returns_sim,c='blue', marker='P',s=200, label='Minimum Volatility')
ax.set(xlabel='Volatility', ylabel='Expected Returns',title='Efficient Frontier')
ax.legend()

"""#Finding the Efficient Frontier with Optimization

min $w^T \sum w $ \
s.t. w^T \b{1} = 1 \
$w \geq 0$ \
$w^T \mu = \mu_p$

$w$ is a vector of weights, $\sum$ is the covariance matrix, is a vector of returns, and $\mu_p$ is the expected portfolio return.

We use optimization for finding the optimal portfolio weights over a range of expected portfolio returns.
"""

def get_portf_rtn(w, avg_rtns):
    return np.sum(avg_rtns * w)

def get_portf_vol(w, cov_mat):
    return np.sqrt(np.dot(w.T, np.dot(cov_mat, w)))

N_POINTS = 1000
portf_vol_ef = []
indices_to_skip = []

portf_rtns_ef = np.linspace(portf_results_sim_df.returns_sim.min(),portf_results_sim_df.returns_sim.max(),N_POINTS)

portf_rtns_ef = np.round(portf_rtns_ef, 2)
rtns_range = portf_rtns_ef

efficient_portfolios = []
n_assets = len(avg_returns)
args = (cov_mat)
bounds = tuple((0,1) for asset in range(n_assets))
initial_guess = n_assets * [1. / n_assets, ]
for ret in portf_rtns_ef:
    constraints = ({'type':'eq','fun': lambda x: get_portf_rtn(x, avg_returns)- ret},{'type':'eq','fun': lambda x: np.sum(x) - 1})
    efficient_portfolio=sco.minimize(get_portf_vol,initial_guess,args=args,method='SLSQP',constraints=constraints,bounds=bounds)
    efficient_portfolios.append(efficient_portfolio)

vols_range = [x['fun'] for x in efficient_portfolios]

fig, ax = plt.subplots()
fig.set_figheight(8)
fig.set_figwidth(15)
portf_results_sim_df.plot(kind='scatter', x='volatility_sim', y='returns_sim', c='sharpe_ratio_sim', cmap='RdYlGn', edgecolors='black', ax=ax)
ax.plot(vols_range, rtns_range, 'b--', linewidth=3)
ax.set(xlabel='Volatility',ylabel='Expected Returns',title='Efficient Frontier')

min_vol_ind = np.argmin(vols_range)
min_vol_portf_rtn = rtns_range[min_vol_ind]
min_vol_portf_vol = efficient_portfolios[min_vol_ind]['fun']
min_vol_portf = {'Return': min_vol_portf_rtn,'Volatility':
                 min_vol_portf_vol,'Sharpe Ratio': (min_vol_portf_rtn /min_vol_portf_vol)}

print('Minimum volatility portfolio ----')
print('Performance')
for index, value in min_vol_portf.items():
    print(f'{index}: {100 * value:.2f}% ', end="", flush=True)

print('\nWeights')
for x, y in zip(RISKY_ASSETS,efficient_portfolios[min_vol_ind]['x']):
    print(f'{x}: {100*y:.2f}% ', end="", flush=True)

max_sharpe_ind = np.argmax(rtns_range/vols_range)
max_sharpe_portf_rtn = rtns_range[max_sharpe_ind]
max_sharpe_portf_vol = efficient_portfolios[max_sharpe_ind]['fun']
max_sharpe_portf = {'Return': max_sharpe_portf_rtn,'Volatility':
                 max_sharpe_portf_vol,'Sharpe Ratio': (max_sharpe_portf_rtn /max_sharpe_portf_vol)}

print('Max Sharpe portfolio ----')
print('Performance')
for index, value in max_sharpe_portf.items():
    print(f'{index}: {100 * value:.2f}% ', end="", flush=True)

print('\nWeights')
for x, y in zip(RISKY_ASSETS,efficient_portfolios[max_sharpe_ind]['x']):
    print(f'{x}: {100*y:.2f}% ', end="", flush=True)

"""#Risk Parity Portfolio

A portfolio that gives higher weight in less risky assets.
"""

def calculate_risk_contribution(weights, covariance_matrix):
    portfolio_std = np.sqrt(np.dot(weights.T, np.dot(covariance_matrix, weights)))
    marginal_risk_contribution = np.dot(covariance_matrix, weights) / portfolio_std
    risk_contribution = weights * marginal_risk_contribution
    return risk_contribution

def risk_parity_portfolio_cov(returns):
    covariance_matrix = returns.cov().values * 52

    n_assets = returns.shape[1]
    initial_weights = np.ones(n_assets) / n_assets

    def objective(weights):
        risk_contributions = calculate_risk_contribution(weights, covariance_matrix)
        return np.std(risk_contributions)

    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})

    bounds = tuple((0, 1) for asset in range(n_assets))

    result = minimize(objective, initial_weights, method='SLSQP', bounds=bounds, constraints=constraints)

    if not result.success:
        raise ValueError("Optimization failed")

    if isinstance(returns, pd.DataFrame):
        return pd.Series(result.x, index=returns.columns)
    else:
        return result.x

asset_returns = returns_train
rppWeights = risk_parity_portfolio_cov(asset_returns)
print(rppWeights)

plt.figure(figsize=(30,16))
y1 = rppWeights
y2 = efficient_portfolios[min_vol_ind]['x']

X = np.arange(15)
plt.bar(X , y1, color = 'b', width = 0.25)
plt.bar(X + 0.25, y2, color = 'r', width = 0.25)

Labels = RISKY_ASSETS
plt.xticks(X, Labels)
plt.legend(['RPP','Min. Var.'])
plt.xlabel('Asset')
plt.ylabel('Portfolio Weight')
plt.title('Asset Allocation')

plt.show()

plt.figure(figsize=(30,16))
y1 = calculate_risk_contribution(rppWeights, cov_mat)
min_var_weights = efficient_portfolios[min_vol_ind]['x']
max_sharpe_weights=efficient_portfolios[max_sharpe_ind]['x']
y2 = calculate_risk_contribution(min_var_weights, cov_mat)

X = np.arange(15)
plt.bar(X , y1, color = 'b', width = 0.25)
plt.bar(X + 0.25, y2, color = 'r', width = 0.25)

Labels = RISKY_ASSETS
plt.xticks(X, Labels)
plt.legend(['RPP','Min. Var.'])
plt.xlabel('Asset')
plt.ylabel('Risk Contributions')
plt.title('Risk Contributions')

plt.show()

"""#Setting Volatility to 0.05, 0.10, 0.15"""

import numpy as np
import pandas as pd

target_volatilities = [0.05, 0.10, 0.15]
weights_5=[]
weights_10=[]
weights_15=[]

for target_volatility in target_volatilities:

  weights = np.array(max_sharpe_weights)
  portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov_mat, weights)))
  scaling_factor = target_volatility / portfolio_std
  adjusted_weights = weights * scaling_factor
  adjusted_portfolio_std = np.sqrt(np.dot(adjusted_weights.T, np.dot(cov_mat, adjusted_weights)))


  if target_volatility == 0.05:
     weights_5_max_sharpe_weights = adjusted_weights
  elif target_volatility == 0.10:
     weights_10_max_sharpe_weights = adjusted_weights
  elif target_volatility == 0.15:
     weights_15_max_sharpe_weights = adjusted_weights

print("\nAdjusted Weights for target volatilities with max sharpe weights:")
print("weights_5:", weights_5_max_sharpe_weights)
print("weights_10:", weights_10_max_sharpe_weights)
print("weights_15:", weights_15_max_sharpe_weights)

import numpy as np
import pandas as pd

target_volatilities = [0.05, 0.10, 0.15]
weights_5=[]
weights_10=[]
weights_15=[]

for target_volatility in target_volatilities:

  weights = np.array(rppWeights)
  portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov_mat, weights)))
  scaling_factor = target_volatility / portfolio_std
  adjusted_weights = weights * scaling_factor
  adjusted_portfolio_std = np.sqrt(np.dot(adjusted_weights.T, np.dot(cov_mat, adjusted_weights)))


  if target_volatility == 0.05:
     weights_5_rppWeights = adjusted_weights
  elif target_volatility == 0.10:
     weights_10_rppWeights = adjusted_weights
  elif target_volatility == 0.15:
     weights_15_rppWeights = adjusted_weights

print("\nAdjusted Weights for target volatilities with RPP Weights:")
print("weights_5:", weights_5_rppWeights)
print("weights_10:", weights_10_rppWeights)
print("weights_15:", weights_15_rppWeights)

target_volatilities = [0.05, 0.10, 0.15]
weights_5=[]
weights_10=[]
weights_15=[]

for target_volatility in target_volatilities:

  weights = np.array(min_var_weights)
  portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov_mat, weights)))
  scaling_factor = target_volatility / portfolio_std
  adjusted_weights = weights * scaling_factor
  adjusted_portfolio_std = np.sqrt(np.dot(adjusted_weights.T, np.dot(cov_mat, adjusted_weights)))

  if target_volatility == 0.05:
     weights_5_min_var_weights = adjusted_weights
  elif target_volatility == 0.10:
     weights_10_min_var_weights = adjusted_weights
  elif target_volatility == 0.15:
     weights_15_min_var_weights = adjusted_weights

print("\nAdjusted Weights for target volatilities with Min Var Weights:")
print("weights_5:", weights_5_min_var_weights)
print("weights_10:", weights_10_min_var_weights)
print("weights_15:", weights_15_min_var_weights)

target_volatilities = [0.05, 0.10, 0.15]
weights_5=[]
weights_10=[]
weights_15=[]

for target_volatility in target_volatilities:

  weights = np.array(portfolio_weights)
  portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov_mat, weights)))
  scaling_factor = target_volatility / portfolio_std
  adjusted_weights = weights * scaling_factor
  adjusted_portfolio_std = np.sqrt(np.dot(adjusted_weights.T, np.dot(cov_mat, adjusted_weights)))

  if target_volatility == 0.05:
     weights_5_portfolio_weights = adjusted_weights
  elif target_volatility == 0.10:
     weights_10_portfolio_weights = adjusted_weights
  elif target_volatility == 0.15:
     weights_15_portfolio_weights = adjusted_weights

print("\nAdjusted Weights for target volatilities with Rebalancing 1/N Weights:")
print("weights_5:", weights_5_portfolio_weights)
print("weights_10:", weights_10_portfolio_weights)
print("weights_15:", weights_15_portfolio_weights)

"""#Data-Second Period"""

returns_test.describe()

returns_train_yearly_mean=returns_train.mean()
returns_train_yearly_mean

returns_test = returns_test[returns_train.columns]
pricesData_test=pricesData_test[returns_train.columns]
returns_test_yearly_mean=returns_test.mean()
returns_test_yearly_mean

combined_returns = pd.DataFrame({
    'Returns2 Yearly Mean': returns_test_yearly_mean,
    'Returns Yearly Mean': returns_train_yearly_mean
})

print(combined_returns)

"""#Weights"""

optimal_weights = pd.DataFrame({
                                'equal': portfolio_weights,
                                'min var': min_var_weights,
                                'rpp':rppWeights,
                                'max sharpe':max_sharpe_weights,
                                'volatility 5 equal weights' : weights_5_portfolio_weights,
                                'volatility 10 equal weights' : weights_10_portfolio_weights,
                                'volatility 15 equal weights' : weights_15_portfolio_weights,
                                'volatility 5 min var weights' : weights_5_min_var_weights,
                                'volatility 10 min var weights' : weights_10_min_var_weights,
                                'volatility 15 min var weights' : weights_15_min_var_weights,
                                'volatility 5 rpp weights' : weights_5_rppWeights,
                                'volatility 10 rpp weights' : weights_10_rppWeights,
                                'volatility 15 rpp weights' : weights_15_rppWeights,
                                'volatility 5 max sharpe weights' : weights_5_max_sharpe_weights,
                                'volatility 10 max sharpe weights' : weights_10_max_sharpe_weights,
                                'volatility 15 max sharpe weights' : weights_15_max_sharpe_weights,
                               },
                               index=returns_test.columns)

optimal_weights.style.format('{:.4f}')

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

columns = optimal_weights.columns

for i, column in enumerate(columns):
    plt.figure(figsize=(36, 36))
    plt.subplot(4, 4, i + 1)
    sns.barplot(x=optimal_weights.index, y=optimal_weights[column], palette="Blues_d")
    plt.title(f'{column}')
    plt.xticks(rotation=90)


    plt.tight_layout()
    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)


    plt.savefig(f'{column}_barplot_weights.png', bbox_inches='tight')
    plt.show()

"""#Portfolio Allocation"""

def portfolio_allocation(prices, weights, investment=1_000_000, rd=0):
    shares = (weights * investment).div(pricesData_test.iloc[0], axis=0)
    if rd == 1:
        shares = np.floor(shares)

    return pricesData_test.dot(shares)

portfolios = portfolio_allocation(pricesData_test.loc[returns_test.index], optimal_weights, rd=1)
rounded_num = round(portfolios, 4)
rounded_num

"""#Risk Free Rate from Fama French"""

ff_url = "https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_Factors_CSV.zip"

urllib.request.urlretrieve(ff_url,'fama_french.zip')
zip_file = zipfile.ZipFile('fama_french.zip', 'r')

zip_file.extractall()

zip_file.close()

import pandas as pd
import numpy as np

ff_factors = pd.read_csv('F-F_Research_Data_Factors.CSV', skiprows=3, index_col=0)

ff_row = ff_factors.isnull().any(axis=1)
itemIndex = np.where(ff_row)[0]

ff_factors = pd.read_csv('F-F_Research_Data_Factors.CSV', skiprows=3, nrows=itemIndex[0], index_col=0)

ff_factors.index = pd.to_datetime(ff_factors.index, format='%Y%m')

ff_factors.index = ff_factors.index + pd.offsets.MonthEnd()

ff_factors = ff_factors.apply(lambda x: x / 100)

ff_factors_filtered = ff_factors[(ff_factors.index > '2024-02-01') & (ff_factors.index < '2024-10-06')]
ff_factors_filtered=ff_factors_filtered/52
ff_factors_filtered

train_VT, test_VT = [yf.download('VT', start=start, end=end, interval="1wk")["Adj Close"]
                                    for start, end in [("2021-11-01", "2024-03-01"), ("2024-03-01", "2024-11-01")]]

returns_train_VT = train_VT.pct_change().dropna()
returns_test_VT = test_VT.pct_change().dropna()
returns_test_VT_filtered = returns_test_VT[(returns_test_VT.index > '2024-03-03') & (returns_test_VT.index < '2024-10-06')]
returns_test_VT_filtered

returns_test_filtered = returns_test[(returns_test.index > '2024-03-03') & (returns_test.index < '2024-10-06')]
returns_test_filtered

import pandas as pd

returns_test_filtered.index = returns_test_filtered.index.tz_localize(None)
returns_test_VT_filtered.index=returns_test_VT_filtered.index.tz_localize(None)

if ff_factors_filtered.index.tzinfo is not None:
    ff_factors_filtered.index = ff_factors_filtered.index.tz_localize(None)

ff_factors_weekly = ff_factors_filtered['RF'].resample('W-MON').ffill()
ff_factors_weekly

ff_factors_weekly_filtered = ff_factors_weekly[(ff_factors_weekly.index > '2024-03-03') & (ff_factors_weekly.index < '2024-10-06')]

print(ff_factors_weekly_filtered)

excess_returns_test = pd.merge(returns_test_filtered, ff_factors_weekly_filtered, how='inner', left_index=True, right_index=True)
excess_returns_test_VT=pd.merge(returns_test_VT_filtered, ff_factors_weekly_filtered, how='inner', left_index=True, right_index=True)

excess_returns_test_VT=excess_returns_test_VT['VT']- excess_returns_test_VT['RF']
for column in returns_test.columns:
    excess_returns_test[column] = excess_returns_test[column] - excess_returns_test['RF']


excess_returns_test= excess_returns_test.drop(columns=['RF'])
excess_returns_test_VT=excess_returns_test_VT.drop(columns=['RF'])

excess_returns_test_VT

portfolios_filtered = portfolios[(portfolios.index > '2024-02-20') & (portfolios.index < '2024-10-06')]
portfolios_filtered

returns_portfolio = portfolios_filtered.pct_change().dropna()
returns_portfolio

excess_returns_test_portf = pd.merge(returns_portfolio, ff_factors_weekly_filtered, how='inner', left_index=True, right_index=True)
for column in returns_portfolio.columns:
    excess_returns_test_portf[column] = returns_portfolio[column] - excess_returns_test_portf['RF']

excess_returns_test_portf= excess_returns_test_portf.drop(columns=['RF'])

excess_returns_test_portf

"""#Performance Metrics"""

for portfolio in returns_portfolio.columns:

  portfolio_std = np.std(returns_portfolio[portfolio])
  sharpe_ratio = excess_returns_test_portf.mean() / portfolio_std


  slope, intercept, r_value, p_value, std_err = linregress(excess_returns_test_VT, excess_returns_test_portf[portfolio])
  beta = slope
  alpha = intercept

  tracking_error = np.std(excess_returns_test_portf - excess_returns_test_VT*beta-alpha)
  information_ratio = alpha / tracking_error

  treynor_ratio = excess_returns_test_portf.mean() / beta

def get_alpha(series, excess_returns):
    slope, intercept, r_value, p_value, std_err = linregress(excess_returns, series)
    return intercept

def get_beta(series, excess_returns):
    slope, intercept, r_value, p_value, std_err = linregress(excess_returns, series)
    return slope

def get_residuals(series, excess_returns):
  slope, intercept, r_value, p_value, std_err = linregress(excess_returns, series)
  return (excess_returns - series * slope - intercept).std()

def get_r_value(series,excess_returns):
  slope, intercept, r_value, p_value, std_err = linregress(excess_returns, series)
  return r_value

stats_df = pd.DataFrame({
    'mean': returns_portfolio.mean(),
    'std': returns_portfolio.std(),
    'sharpe': excess_returns_test_portf.mean() / returns_portfolio.std(),
    'alpha': returns_portfolio.apply(get_alpha, excess_returns=excess_returns_test_VT),
    'beta': returns_portfolio.apply(get_beta, excess_returns=excess_returns_test_VT),
    'residuals': returns_portfolio.apply(get_residuals, excess_returns=excess_returns_test_VT),
    'r_value': returns_portfolio.apply(get_r_value,excess_returns=excess_returns_test_VT)
    })

stats_df['information'] = stats_df['alpha'] / stats_df['residuals'].std()
stats_df['treynor'] = excess_returns_test_portf.mean() / stats_df['beta']

stats_df

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

columns = ['mean', 'std', 'sharpe', 'alpha', 'beta', 'residuals', 'r_value', 'information', 'treynor']

for i, column in enumerate(columns):
    plt.figure(figsize=(30, 24))
    plt.subplot(3, 3, i + 1)
    sns.barplot(x=stats_df.index, y=stats_df[column], palette="Blues_d")
    plt.title(f'{column.capitalize()}')
    plt.xticks(rotation=90)

    plt.tight_layout()
    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)


    plt.savefig(f'{column}_barplot_models.png', bbox_inches='tight')
    plt.close()